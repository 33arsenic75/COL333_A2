\begin{quote}
moves with high winning rate should be exploited more, but moves with a
small number of simulations as compared to the parent should be explored
to improve the confidence. This formula is guaranteed to converge to a
best move given infinite time and memory.

\textbf{2.4.2} \textbf{RAVE: Rapid Action Value Estimate}

In basic MCTS many thousands of simulations are usually run per second,
but the information about which moves were made during the rollouts is
unused. A win or a loss is composed of many moves which contribute to
that outcome, and often good moves during a rollout are also good moves
if made earlier during the rollout or descent phases. This is a similar
to the reasoning behind the history heuristic. Thus, we can keep a
winning rate for each move during the rollouts and use this to encourage
exploration of moves that do well during rollouts. This winning rate is
called the Rapid Action Value Estimate (RAVE) {[}9, 10{]}. RAVE
experience is gathered more quickly than by pure experience alone,
though it is less correlated to success, and so should be phased out as
real experience is gained. For a given node \emph{n}, \emph{n.r} is the
RAVE winning rate and \emph{n.m} is the number of RAVE updates.
\end{quote}

20

\begin{quote}
Chapter 2: Background

Usually RAVE experience and real experience are combined as a linear
com-bination, starting as only RAVE experience and asymptotically
approaching only real experience. This combination replaces \emph{ni.v}
in Equation 2.4.1:

\emph{β ⇤ni.v} + (1 \emph{−β}) \emph{⇤ni.r} (2.4.2)
\end{quote}

Several formulas for \emph{β} have been proposed. The simplest two
formulas for \emph{β}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
are:\\
\emph{k}\\
\emph{β} = (2.4.3) \emph{k} + \emph{ni.n}

\emph{β} = r \emph{k} + 3 \emph{⇤ni.n} \emph{k} (2.4.4)
\end{quote}

both of which have a tunable constant \emph{k} which represents the
midpoint, the\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
number of simulations needed for the RAVE experience and real experience
to have equal weight.

David Silver computed an optimal formula for \emph{β} under the
assumption of independence of estimates {[}11{]}:
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{ni.m}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule()
\endhead
\emph{β} = & \emph{ni.n} + \emph{ni.m} + 4 \emph{⇤ni.n ⇤ni.m ⇤b}2 &
(2.4.5) \\
\bottomrule()
\end{longtable}

\begin{quote}
where \emph{b} is a tunable RAVE bias value.

In practice, RAVE leads to a large increase in playing strength for
games such as Go and Havannah where the assumption that a good move is
also good if played earlier holds. The RAVE updates often lead to
sufficiently large exploration that the constant in the UCT exploration
term is set very low or even to 0, removing UCT exploration altogether.

\textbf{2.4.3} \textbf{Heuristic Knowledge}

While UCT is guaranteed to converge given infinite time, game specific
knowl-edge can encourage it to find good moves faster. When a node is
expanded, its
\end{quote}

21

\begin{quote}
Chapter 2: Background

children all start with no experience, so the default policy is to
choose between them randomly. The simulation is more representative of a
good game, and leads to a better understanding of the minimax value, if
it chooses a good move first. Eventually the best move will receive the
majority of the simulations, and we'll do better if this is true right
from the beginning. Each game has its own heuristics, and
Havannah-specific ones are described in later chapters, but the way
these heuristics are used is game independent.

The first way heuristic knowledge is used is to simply add fake
experience to a node. Instead of initializing a node as \emph{ni.v} =
0\emph{, ni.n} = 0, good moves can be initialized with \emph{ni.v} =
\emph{a, ni.v} = \emph{b}, where \emph{a} and \emph{b} are tunable
constants, which e↵ectively means that this node has some amount of wins
attributed to it before any simulations have gone through it. This has
the e↵ect of allowing the node to look good for the first while even if
it is unlucky. The extra simulations will fade over time as the few
extra wins becomes insignificant in the long run. Bad moves can
similarly be initialized with fewer wins than simulations, e↵ectively
depressing its early winning rate. Depending on the implementation, this
may encourage the first few simulations to avoid the good moves, due to
their smaller confidence bounds compared to similar moves with the same
high winning rate. This has the e↵ect of making the grandparent move
look bad. This knowledge could also be added as fake RAVE experience as
well as, or instead of, actual experience.

Another way heuristic knowledge is used is to add a knowledge term to
the value formula. This leaves the experience and confidence bounds
alone, but gives a boost for the first few simulations to nodes with
higher knowledge. This has the added benefit of being able to order the
nodes by boost size. The knowledge term should fall o↵with increasing
experience. Three suggested knowledge terms are:
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{ni.k}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{,}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{ni.k}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{,}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{ni.k}
\end{minipage} \\
\midrule()
\endhead
\emph{log}(\emph{ni.n}) & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{pni.n}
\end{quote}
\end{minipage} & \emph{ni.n} \\
\bottomrule()
\end{longtable}

\begin{quote}
where \emph{ni.k} is the knowledge value for the node \emph{ni}.
\end{quote}

22

\begin{quote}
Chapter 2: Background

\textbf{2.4.4} \textbf{Rollout Policy}

The strength of MCTS is highly dependent on the average outcome of the
rollouts being representative of the strength of the position. When a
player who is in a good position has an easy defence to a devastating
attack, but fails to defend, the outcome is not representative of the
strength of the original position. Decreasing randomness by enforcing
defences against devastating attacks can bias the outcome, but usually
leads to higher quality and more representative games, leading to a
stronger player. Most rollout policies used in real programs are game
specific, but a few game independent ones are mentioned here.

Instead of pure random, a weighted random scheme can be used. Moves that
have good experience in the tree can be selected with a higher
probability to poor moves. This could be based on real experience, RAVE
experience, pattern knowledge or heuristic knowledge as described in the
Section 2.4.3.

The Last Good Reply {[}12, 13{]} scheme can be used, where the moves
made by the player that won a rollout are saved for use in later
rollouts when similar situations occur. When these moves fail to lead to
a win in a later rollout, they may be removed from the list of replies.

All possible moves can be checked to see if they lead to an instant win
if made, or an instant loss if made by the opponent. If a winning moves
exists, it should be made, and if the opponent has a winning move, it
should be blocked.

\textbf{2.5 Summary}

Several game playing and solving algorithms exist, but they're all based
on minimax. Minimax chooses the move that minimizes the maximum outcome
the opponent can achieve.
\end{quote}

23

\begin{quote}
Chapter 2: Background

Alpha-beta is a refinement to minimax that prunes parts of the tree that
can't a↵ect the minimax value of the root. Transposition tables reduce
the search space from a tree to a graph, reducing the search space.
Iterative deepening, allows an early result to be returned, and combined
with transposition tables, gives better move ordering allowing deeper
searches. The history heuristic also improves move ordering.

Proof number search is an algorithm for solving the outcome of games. It
maintains estimates of the difficulty of solving a subtree, preferring
to solve easier parts of the tree. This leads to it preferring to
explore forced moves and slim parts of the tree. A transposition table
can be used to reduce the search space and solve problems that are
bigger than physical memory.

Monte-Carlo Tree Search is a game playing algorithm that works well on
prob-lems where no good heuristic is known. It consists of four phases:
descent, expansion, rollout and back-propagation. It chooses a leaf
node, grows the tree, plays a random sequence of moves, and uses the
outcome of this random game to bias the next descent. MCTS can be
improved by choosing a good balance between exploration and
exploitation. Gaining experience from the moves made within rollouts can
be a big help, as can biasing the descent to-wards better moves based on
heuristic knowledge. A rollout policy that leads to outcomes that are
more representative of the true outcome is also useful.
\end{quote}

24

\end{document}
